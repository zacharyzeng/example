{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get semi-structured data: Web scraping [获取半结构化的数据：网页抓取]\n",
    "\n",
    "\n",
    "## A word on unified environment [统一环境上同样的字]\n",
    "\n",
    "### virtualenv(创建虚拟化python环境), Python3 and Python2\n",
    "    Python3是python现在的标准版本。在Python3广泛传播前，这个社区花了大约十年时间的努力。标准和解释器核心离开市场了很长时间，但是许多有用的libraries都等着支持Python3。人们建立了一道耻辱之墙，之后却变成了\"超级大国之墙\"，去追踪移向Python3的进展。这都是因为Python3不是相对于Python2向后兼容的升级版。从Python2升级到Python3以后，过去的代码会无效/中止。\n",
    "    \n",
    "#### TIP: 如果现在开始一个新的项目，使用Python3, *Python3*, Python3\n",
    "\n",
    "    然而，在一些地方，特别是老的系统或环境里，仍然有Python2，或者依赖于Python2。为了让事情不凌乱，请安装virtualenv以及在virtualenv安装Jupyter notebook。\n",
    "\n",
    "### Jupyter notebook\n",
    "    Windows使用者和Mac OS X使用者在1-3章里经历了不同的环境设置问题。从第4章开始，所有的工作将基于virtualenv和Jupyter notebook完成。所以，每个人在将来都会看见同样的输入/输出。\n",
    "    A big cheers! -- you have already passed through the most difficult part of the whole course.（还有点懵...）\n",
    "    欲知更多有关Jupter的信息，详见\"FQA on Jupyter\"(Open book里有链接)\n",
    "\n",
    "\n",
    "## Knowledge about HTML [HTML的知识]\n",
    "\n",
    "### Chrome DevTools Chrome [开发者工具]\n",
    "    Chrome开发者工具是一套直接内建在谷歌Chrome浏览器里的网站开发者工具。对于我们来说，Chrome开发者工具能够帮助更好学习观看甚至改变一个页面代码的基础，我们便可更好地理解到一个网页的结构，一个网站如何存储数据，呈现信息以及最重要的是，我们如何定位/找到想要的信息并且提取它们到结构化的数据以进行更深入的分析。\n",
    "    \n",
    "### How to use Chrome DevTools [如何使用Chrome开发者工具]\n",
    "    1.首先，建议使用Chrome作为浏览器\n",
    "    2.在Chrome里，'option+command+i'键可以打开Chrome开发者工具，也叫作Chrome开发者控制台(Chrome developer console)\n",
    "    3.点击控制台的左上角，可以在网页里选择一个元素去审视/检查它。可以通过移动光标(cursor)到其之上看它的源代码。\n",
    "\n",
    "比如: 查看一个网页的结构, 一个关于特朗普推文的项目: https://initiumlab.com/blog/20170329-trump-and-ivanka/. 比如，移动光标去查看每个H2标题。\n",
    "\n",
    "### Frontend three: HTML, JS, and CSS [前端3佬:HTML, JS, CSS]\n",
    "    · HTML是一种网页的机器语言。在HTML里写一些东西意味着创建一个网页。它是一个多种标签的结构。这些标签都是成对出现，通过开标签和闭标签包裹住我们想要呈现的内容。比如，<p> content </p> 。\n",
    "    · CSS代表Cascading Style Sheets(层叠式样表)。CSS描述HTML元素如何被展示在屏幕，纸或者其他媒体上。\n",
    "    · JavaScrip是HTML和网站的编程语言，主要用于图形操作，表格确认以及内容的动态改变。\n",
    "    \n",
    "### HTML\n",
    "    HTM是一种“说明性语言”，然而Python是“解释性语言”(总而言之，宽松的条件)。这里的关键不同在于“说明性语言”不能指示机器如何去解决一个问题/一步一步形式的呈现结果。相反，它告诉机器想要的输出结果是什么，以及它受机器的支配如何用其自己的方式产出结果。\n",
    "    \n",
    "    整个互联网都依赖于HTML，所以能找到大量的免费在线资源深入学习HTML，比如，在这里。我们的主要目的不是教授每一个人去写网页(前端发展)。我们着重于理解HTML页面，以及从句法上分析来自页面里有用的结构数据。\n",
    "    下面是HTML的核心概念:\n",
    "#### · 它是一种基于标记的语言。常用标记可以是“p, h1, h2, h3, ul, ol, li, img, a”\n",
    "#### · 标记是成对出现，并且按照嵌套顺序: \n",
    "      #Paired# —— 如果看见<p>,那之后肯定会有一个</p>；文件看起来就像:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1251151fbefe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1251151fbefe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <p>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<p>\n",
    "  ... content is here ...\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      #Nested# —— 相信一个list的购物商品在页面上，每一个商品都是一个图片链接，当点击的时候会带到详情页面。这个结构就像：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<ul>\n",
    "  <li>\n",
    "      <a href=\"link to the destination when click\">\n",
    "          <img src=\"link to the thumbnail image\">\n",
    "      </a>\n",
    "  </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    这样，便可以建立一个网页的复杂结构。里面的标记叫作外面标记的“children”(小孩)。也可以在心中描绘出HTML文件像大树的结构一样，“<html></html>”是“root”，然后每一个其他的标记是一个“branch”，“branch”又有其“child”标记和“parent”标记。值得注意的是，在上述例子中，图片标记并没有成对出现。这是因为不能再图片中放入任何内容。它就像是一片“leaf”，我们不能再在它里面附上标记。朝着这个目标，我们能简略的删去闭合标记“</img>”。这曾经是一个很好的方式使得标记成对出现。因为”HTML5“的创始，它则被建议保留成“leaf”标记本来的形式，比如，没有成对的闭合标记以及里面没有内容。\n",
    "    为了便于讨论，我们也称呼“HTML标记”为“HTML element”(HTML元素)或者“HTML node”(HTML节点)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "\n",
    "### Basic logic [基本逻辑]\n",
    "    在尝试去获取数据之前，这是我们需要知道的逻辑。基本来说，当我们爬取一个网站，首先我们需要知道这个网站。\n",
    "    1.这个网站是否提供它自己的API去获取数据？\n",
    "    2.如果没有，我们将从它的html来抓取数据。\n",
    "    3.所有数据或信息被存储在html标记里。标记名称由网站创建者设置，通常都是成对出现。所以，我们需要去做的就是找到包含我们需要数据的标记(tags)。比如，文章标题通常是在“h1”里，文本通常是“p”里。可以通过Chrome开发者工具找到这些标记。\n",
    "    4.在我们找到数据和标记之后，我们需要写代码去获取它们(使用control flows)，清除它们(操作字符串，strip()，replace()...)，然后把它们存储进文件(CSV,JSON)。\n",
    "    \n",
    "### New module: BeautifulSoup [新的moudle: BeautifulSoup]\n",
    "#### “bs4”是“BeautifulSoup4”的缩写。Beautiful Soup是一个为了把数据从HTML和XML文件中取出的Python library。\n",
    "    换句话说，BeautifulSoup按句法分析要求到结构性数据的html的内容，以便我们能轻松地找到我们想要的元素。\n",
    "\n",
    "    安装BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    引入moudle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use BeautifulSoup parser [使用BeautifulSoup句法分析器]\n",
    "    BeautifulSoup句法分析器能够我们请求的结果转换到结构性数据，以便我们能更方便地找到我们想要的数据。\n",
    "    例如: https://initiumlab.com/blog/20170329-trump-and-ivanka/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "    #print(r) 将会得到 <Response [200]> 表示请求成功\n",
    "html_str = r.text\n",
    "    #获取请求的内容\n",
    "    \n",
    "    ·存储网站作为“r”，“get()”表示尝试去得到网页的回应，传递url字符串到“()”里面。\n",
    "    ·“text”表示展示 网页的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "html_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #输出: 这是进行句法分析步骤之前，可以 它们很混乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BeautifulSoup(html_str,\"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意它的句法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "data = BeautifulSoup(html_str,\"html.parser\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #输出: 进行句法分析后，能看见数据变得更结构化，我们便能够通过使用“control flows”和“[]”，“{}”的操作，更深入地得到/找到数据。\n",
    "    \n",
    "### Find data: find.() and find_all() [Find数据: find.() 和 find_all()]\n",
    "    ·“find”是为了找到我们想要的东西，然后输出第一个对象。比如，如果“10 h1”，它们会返回到第一个\n",
    "    ·“find_all”是返回一个包含所有我们想要的values的list。比如，“10 h1”，它们会返回一个包含所有这些h1的list。一个list表示我们能使用“for”循环去做更进一步的过滤。\n",
    "\n",
    "Example:\n",
    "html_doc is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "my_a = soup.find('a')\n",
    "my_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_a = soup.find_all('a')\n",
    "my_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "my_a = soup.find('a',attrs={'id':'link3'}) \n",
    "my_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_a = soup.find('a',attrs={'id':'link3'}) \n",
    "\n",
    "    #Note: 可以看见在标记“a”里，有一些“attributes”(attrs)，比如“class”，”id“。这些”attributes“是被用作从其它类似的标记中区分这个标记，特别是当html页面里有许多标记的时候。所以，如果你想要去更准确地定位或者找到某些东西。可以发现专门找到这些”attributes“通过把它们以”soup.find('tag_name',attrs={'attributes':'values'})“写入。\n",
    "    \n",
    "    通常，parser(句法分析器)和find功能是BeautifulSoup library里最常备我们用到的。\n",
    "\n",
    "\n",
    "\n",
    "### Get data [获取数据]\n",
    "#### Get title [获取标题]\n",
    "    打开Chrome开发者工具，通过移动鼠标到标题，可以发现标题是在:\n",
    "    Open the chrome devtools, by moving the mouse on the headline, you can find title is in:\n",
    "<h1 class=\"post__title\" itemprop=\"name headline\"> 特朗普父女推特解密</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "data = BeautifulSoup(html_str,\"html.parser\")\n",
    "my_h1 = data.find('h1')\n",
    "my_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_title = my_h1.text\n",
    "my_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_title.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    my_h1 = data.find('h1')\n",
    "    ## 使用标记和”attributes“去提取我们想要的数据。”Type(my_h1)“可以看见`my_h1` 是 ”bs4.element.Tag“\n",
    "    my_title = my_h1.text \n",
    "    ## 把bs4.element.Tag变成纯文本\n",
    "    my_title.strip() \n",
    "    ## 在字符串的首尾去掉特定的字符\n",
    "    \n",
    "学习每一步骤的逻辑和function\n",
    "\n",
    "    Note:\n",
    "    ·“strip()”表示删除首尾没有意义的字符。\n",
    "    ·“HTML/bs4_tag.text”表示把“bs4.element.Tag”变成纯文本\n",
    "    ·可以输入“help(str.strip)”查看“strip”的使用\n",
    "    ·“type(sth)”是去打印/展示sth的格式是什么。它的用处在于你应该知道它返回的数据是什么，从而进一步提取我们想要的“values”。比如，如果它是一个“list”，我们应该首先使用“index”去获取它的“value”。同样地，如果它是一个”dict“，我们应该使用”keys“去获取它的”value“。\n",
    "\n",
    "\n",
    "#### Get date [获取日期]\n",
    "    我们使用同样的方式去找到标题，我们能发现时间标记如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<time itemprop=\"dateCreated\" datetime=\"2017-03-29T.....\" content=\"2017-03-29\">\n",
    "                  2017-03-29\n",
    "                </time>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    提取时间value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "data = BeautifulSoup(html_str,\"html.parser\")\n",
    "my_h1 = data.find('h1')\n",
    "my_date = data.find('time').text.strip()\n",
    "my_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过程是:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "data = BeautifulSoup(html_str,\"html.parser\")\n",
    "my_h1 = data.find('h1')\n",
    "my_date = data.find('time')\n",
    "my_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_date.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get author [获取作者]\n",
    "    可以发现作者在”span“里，所以是否只需要使用”find.span”去获取作者就可以了？\n",
    "    \n",
    "方法1: 失败了，因为缺少特性/特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://initiumlab.com/blog/20170329-trump-and-ivanka/')\n",
    "html_str = r.text\n",
    "data = BeautifulSoup(html_str,\"html.parser\")\n",
    "my_authors = data.find('span')\n",
    "my_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这不是我们想要的，合理的猜测是这里有许多“span”。所以要检查有多少“span”在这里，然后找到这些标记之间的不同。“command+f”去打开控制面板的搜索条，然后输入“span”。就能看见这里有超过2个“span”。因此我们应该找到所有的“span”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spans = data.find_all('span')\n",
    "my_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    可以看到“find_all”返回了一个list和许多“span”，作者也出现在其中两个“span”中。所以我们怎么获取他们？我们使用“index”去进入它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "author_1 = my_spans[7].text\n",
    "author_2 = my_spans[8].text\n",
    "authors.append(author_1)  #“append”附加它们到一个list\n",
    "authors.append(author_2)\n",
    "author_1,author_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note: 为什么我们只使用“authors = my_span.find[7:9].text”去找到所有的作者？因为“find[7:9]”或者“find_all”返回元素的一个list，然而使用“().text”function在元素的一个list里，它将报错，这个list没有attribute“text”，这也就意味着它不能被直接地转换成文本。\n",
    "\n",
    "    TIP: “specificity”(特征)问题在写入抓取器中很常见。通常有许多方式，通常也很简单地区找到我们感兴趣的元素。一个需要努力去确认的是，抓取器不会因为其它我们不感兴趣的元素破坏结果。\n",
    "\n",
    "\n",
    "方法2: Best Current Practice(BCP) -- multiple layers of element lookup(find)\n",
    "\n",
    "     · 这里的逻辑是如果我们不能具体说明在内循环层的一个元素，我们展开去找到只在这个元素里有的不同tag.\n",
    "     · 在HTML里，我们能找到authors 的上层tag是\"td\". 但是这里有很多\"td\". 很难被明确到。所以我们要展开。\n",
    "     · \"td\"的外层是带有一个叫作\"post_authors\"class的\"tr\" tag, 你能发现它是仅用于包裹\"authors\"的特别的tag, 所以尝试通过tag去定位和提取authors名字: \"tr\". 一旦我们找到\"tr\", 我们便能进一步找到\"td\", 接着找到所有在\"td\"中出现的\"span\", 这就是我们想要的。\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_authors = []\n",
    "post_authors = data.find('tr',attrs={'class':\"post__authors\"})\n",
    "#pay attention to its syntax, find('tag_name’,attributes={'key':'value'})\n",
    "my_td = post_authors.find_all('td')\n",
    "my_span = my_td[1].find_all('span')\n",
    "for span in my_span:\n",
    "    my_authors.append(span.text)\n",
    "my_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法3: 通过分割一个较大的文本来按句法分析作者\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_authors = data.find('tr',attrs={'class':\"post__authors\"}).text.strip().replace('\\n',',') #after .text, we got the author names with several characters, we can further use strip and replace to omit those meaning less characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_authors = data.find('tr',attrs={'class':\"post__authors\"})\n",
    "my_authors.text.strip()\n",
    "my_authors.text.strip().replace('\\n',',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    · 句法: find('tag_name,attributes={'key':'value'})\n",
    "    · attrs = attributes. 它包含更多有关HTML tags的细节性的信息，能够帮助我们更好的定位和识别values.\n",
    "    · replace('a','b') 表示用b代替a. 能看见甚至在strip()之后，在lines有一个\\n, 在这样的环境里，我们能使用replace去删掉这些字符。\n",
    "    \n",
    "TIP: 请把这个方法和之前的方法作比较。最好的练习是尽可能地从文本处理中重复，特别是在数据处理途径的上游(更早的阶段)。方法3匆匆一看可能更简单，但是方法2却更稳定，特别是在当更多人加入想要去维持一段代码的大量运行中。splitting text的主要缺点是界定符(这里是\\n)可能也会作为文章的部分内容出现，这可能造成困扰。\n",
    "\n",
    "\n",
    "#### Get tags [获取tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = data.find('tr',attrs={'class':'post__tags'}).text.strip().replace(' ','').replace('\\n\\n\\n\\n\\n','|')\n",
    "#you can find that there are several blanks and escape characters in return my_tags. We can use replace to get off those meaningless characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tags = data.find('tr',attrs={'class':'post__tags'})\n",
    "my_tags.text.strip()\n",
    "my_tags.text.strip().replace(' ','').replace('\\n\\n\\n\\n\\n','|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all articles of one page [抓取一个页面的所有文章]\n",
    "如果想要抓取更多文章，会发现这里有许多重复的工作和逻辑，所以我们最好是定义一个function区抓取更多文章。除此之外，我们所需要做的就是找到所有文章的url，以便我们能使用一个for loop去抓取更多文章。\n",
    "\n",
    "Step 1: 基于上面的例子，我们能定义一个如下的function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_article(article_url):\n",
    "    r = requests.get(article_url).text\n",
    "    data = BeautifulSoup(r,\"html.parser\")\n",
    "    my_title = data.find('h1').text.strip()\n",
    "    my_date = data.find('time').text.strip()\n",
    "    my_authors = data.find('tr',attrs={'class':\"post__authors\"}).text.strip().replace('\\n',',')\n",
    "    my_tags = data.find('tr',attrs={'class':'post__tags'}).text.strip().replace(' ','').replace('\\n\\n\\n\\n\\n','|')\n",
    "    my_url = article_url\n",
    "    return my_title,my_authors,my_date,my_tags,my_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: 获取一个页面所有的urls。\n",
    "\n",
    "可以看见这个页面中有许多文章。所以我们应如何抓取所有文章的特性，包括authors, dates, headlines?\n",
    "首先，我们应该得到这些文章的urls.\n",
    "因此，我们定义另一个function去获取这些urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_urls_of_one_page(article_page_url): #scrape_articles_urls_of_one_page\n",
    "    article_urls = []\n",
    "    r = requests.get(article_page_url).text\n",
    "    data = BeautifulSoup(r,\"html.parser\")\n",
    "    my_urls = data.find_all('a',attrs={'class':'post__title-link js-read-more'}) #find the links\n",
    "\n",
    "    #quiz1: if you read the code of this page, most students will try to find ('a',attrs={'class':'post__title-link'}) first and failed. Do you know why?\n",
    "    #quiz2: you will find that url can be extracted by my_url['href'], the results will be like this: '../blog/20160908-taipei-power-usage/', but the real one should be like this 'http://initiumlab.com/blog/20160908-taipei-power-usage/',\n",
    "    #so who do we format those links we want?\n",
    "\n",
    "    for my_url in my_urls:\n",
    "        url ='{0}{1}'.format('http://initiumlab.com',my_url['href'][2:]) #format urls\n",
    "        #print(url)\n",
    "        article_urls.append(url)\n",
    "\n",
    "    return article_urls\n",
    "scrape_articles_urls_of_one_page('http://initiumlab.com/blog/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在得到一个页面所有的文章urls之后，就可以调用\"scrape_one_article(article_url)\" function去抓取这个页面所有的特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "article_urls = scrape_articles_urls_of_one_page('http://initiumlab.com/blog/') #scrape_articles_urls_of_page1\n",
    "for article_url in article_urls:\n",
    "    articles.append(scrape_one_article(article_url))\n",
    "\n",
    "with open('articles.csv','w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = ['Titles','Authors','Dates']\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Scrape all articles features of all pages [抓取所有页面的所有文章的特性]\n",
    "\n",
    "因为我们抓取了文章的一个页面，那能抓取所有文章的所有页面吗？\n",
    "\n",
    "    潜在的挑战：\n",
    "    · 从不同页面抓取文章urls. 因为不同页面的文章urls形式在改变，加上文章的url是不是我们直接想要的，所以需要我们进一步去构建这些urls.\n",
    "    · 格式化所有页面的urls.\n",
    "    · 了解三层结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #week o4 request module\n",
    "from bs4 import BeautifulSoup #pay attention to its syntax\n",
    "import csv\n",
    "\n",
    "def scrape_one_article(article_url):  #scrape one articles features, which we've already done this\n",
    "    r = requests.get(article_url).text\n",
    "    data = BeautifulSoup(r,\"html.parser\")\n",
    "    my_title = data.find('h1').text.strip()\n",
    "    my_date = data.find('time').text.strip()\n",
    "    my_authors = data.find('tr',attrs={'class':'post__authors'}).text.strip().replace('\\n',',')\n",
    "    my_tags = data.find('tr',attrs={'class':'post__tags'}).text.strip().replace(' ','').replace('\\n\\n\\n\\n\\n','|')\n",
    "    my_url = article_url\n",
    "    return my_title,my_authors,my_date,my_tags,my_url\n",
    "\n",
    "def scrape_articles_urls_of_one_page(article_page_url): #scrape_articles_urls_of_one_page, its a little bit different from demo above because in the following pages(2-7), the articles' urls are different...\n",
    "    article_urls = []\n",
    "    r = requests.get(article_page_url).text\n",
    "    data = BeautifulSoup(r,\"html.parser\")\n",
    "    my_urls = data.find_all('a',attrs={'class':'post__title-link js-read-more'})\n",
    "    for my_url in my_urls:\n",
    "        url ='{0}blog{1}'.format('http://initiumlab.com/',my_url['href'].split('/blog')[-1]) #format urls.\n",
    "        # Fail try 1 : use slice to cut off ../../..\n",
    "        # Fail try 2 : use blog instead of /blog to split. There are blog in the headline\n",
    "        #print(url)\n",
    "        article_urls.append(url)\n",
    "    return article_urls\n",
    "\n",
    "def scrape_all_pages(url):\n",
    "    articles=[]\n",
    "    for i in range(1,8):  #format all pages urls\n",
    "        if i == 1:\n",
    "            page_url = url\n",
    "        else:\n",
    "            page_url = '{url_initial}page/{number}/'.format(url_initial = url,number=i)\n",
    "            #print(page_url)\n",
    "\n",
    "        article_urls = scrape_articles_urls_of_one_page(page_url)\n",
    "        for article_url in article_urls:\n",
    "            articles.append(scrape_one_article(article_url))\n",
    "\n",
    "    return(articles)\n",
    "\n",
    "with open('initiumlab_articles.csv','w',newline='') as f:\n",
    "    all_articles = scrape_all_pages('http://initiumlab.com/blog/')\n",
    "    writer = csv.writer(f)\n",
    "    header = ['Titles','Authors','Dates','Tags','Url']\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(all_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper pattern [抓取器模式]\n",
    "\n",
    "### Data structure [数据结构]\n",
    "\n",
    "\"list-of-dict\"结构更受欢迎，我们也用这种方式来组织我们的代码:\n",
    "\n",
    "    · 第一(外)层是\"list\"——读写我们感兴趣的数据内容\n",
    "    · 第二(内)层是\"dict\"——提取单一数据内容的形式\n",
    "    \n",
    "\"list-of-dict\"是一个可替代的存储数据的方法。优点是 ##数据入口的压缩形式(?)##。而不是{key1: value1, key2: value2, ..}, 我们是通过[value, value2, ...]. (不重要的)缺点是没有在\"list-of-dict\"中作为\"keys\"形式出现的\"table headers\", 或者\"column names\" (纵向名称)。它能在\"dataset\"之外维持这个信息。\n",
    "\n",
    "数据结构的选择和项目的工作流密切相关。所以，换句话说，它是思考过程的一个反映。\n",
    "\n",
    "\n",
    "\n",
    "### Item-first v.s. attribute-first\n",
    "\n",
    "item-first方法是被认为最佳练习的方式当开始的时候。假设爬取OpenRice网站。每一个事物是一个餐厅，领域包括title、like、location等。假设我们在\"mypage\"里已经有BeautifulSoup的对象。下面是item-first方法的结构: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ef75caa1c707>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ef75caa1c707>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    myitems = mypage.find_all(???)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "myitems = mypage.find_all(???)\n",
    "for myitem in myitems:\n",
    "    title = myitem.find(???).???\n",
    "    like = myitem.find(???).???\n",
    "    location = myitem.find(???).???\n",
    "    ...\n",
    "    # the variables can be None to represent missing data\n",
    "    dataset.append([title, like, location, ...])\n",
    "...\n",
    "csv.writerows(dataset) # one line is enough for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute-first方法不受欢迎，尽管有的时候代码似乎更简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "likes = []\n",
    "locations = []\n",
    "...\n",
    "\n",
    "for e in mypage.find_all(???):\n",
    "    titles.append(e.???)\n",
    "for e in mypage.find_all(???):\n",
    "    likes.append(e.???)\n",
    "for e in mypage.find_all(???):\n",
    "    locations.append(e.???)\n",
    "...\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    csv.writerow(titles[i], likes[i], locations[i], ...)\n",
    "\n",
    "# Or try this more compact method:\n",
    "# csv.writerows(zip(titles, likes, locations, ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing data in scraping [处理在抓取过程中遗漏的数据]\n",
    "\n",
    "在数据处理过程中一个常用的方法是尽可能地在渠道的早期步骤里保存更多的原始信息。下游分析项目总是能决定如何处理遗漏的数据。所以不是用一些\"reasonable value\"来替代遗漏的数据，更好的做法是防止\"None\"在那个地方。\"None\"会成为在CSV里的空格和JSON里的\"null\"去代表\"empty\"。\n",
    "在之后的章节中，当通过\"pandas\"载入数据的时候，数据框架将放置一个\"NaN\"在那个位置，表示\"not a number\"(不是一个)\n",
    "meaning \"not a number\". The None, null, NaN, or empty, are language specific way of treating missing data. No matter which way it adopts, leting the user know the data is missing is important.\n",
    "\n",
    "To further understand this missing data is data philosophy, one can simply mind experiment the \"average\" operation. When the data entry is marked as missing, we just skip this data. However, if someone substituted this missing data in upstream with some reasonable value, say \"0\", the output will be smaller -- \"0\" does not contribute to the numerator but having a valid data entry here contributes 1 more to the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
